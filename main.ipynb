{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook is designed to showcase my thought process for the first part of the challenge. This notebook is purely to help yourself and the wider team easily understand and evaluate the approach taken. This notebook isn't something we'd productionise.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hey team ðŸ‘‹, I hope you're ready for a bit of an adventure as we delve into our first task outlined in the challenge brief.\n",
    "\n",
    "ðŸ‘‰ *Develop some code to compute the similarity metric for each image-text pair and save it in an additional column in the given csv file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylangesmith/projects/leonardo.ai/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.image.image import load_image_from_url\n",
    "from src.image_text_similarity_calculator import ImageTextSimilarityCalculator\n",
    "from src.model.zero_shot_image_classification_model import Input\n",
    "from src.model.openai_clip_vit_model import OpenaiClipVitModel\n",
    "from src.distance.cosine_distance_calculator import CosineDistanceCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Dataset contains 51 rows and 2 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cdn.leonardo.ai/users/85498bb1-9ae7-4b...</td>\n",
       "      <td>2 friendly real estate agent standing. one wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cdn.leonardo.ai/users/b5a9a19e-f630-4e...</td>\n",
       "      <td>vector pattern, pastel colors, in style kawai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://cdn.leonardo.ai/users/925ced00-c573-43...</td>\n",
       "      <td>a young beautiful girl run away kitchen. got s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn.leonardo.ai/users/61b0d7a9-8b0d-46...</td>\n",
       "      <td>CrianÃ§a menino de 1 ano cabelo cacheado com as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://cdn.leonardo.ai/users/566cd98a-7e64-47...</td>\n",
       "      <td>A little girl wearing a red dress smiled. Play...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://cdn.leonardo.ai/users/85498bb1-9ae7-4b...   \n",
       "1  https://cdn.leonardo.ai/users/b5a9a19e-f630-4e...   \n",
       "2  https://cdn.leonardo.ai/users/925ced00-c573-43...   \n",
       "3  https://cdn.leonardo.ai/users/61b0d7a9-8b0d-46...   \n",
       "4  https://cdn.leonardo.ai/users/566cd98a-7e64-47...   \n",
       "\n",
       "                                             caption  \n",
       "0  2 friendly real estate agent standing. one wit...  \n",
       "1  vector pattern, pastel colors, in style kawai ...  \n",
       "2  a young beautiful girl run away kitchen. got s...  \n",
       "3  CrianÃ§a menino de 1 ano cabelo cacheado com as...  \n",
       "4  A little girl wearing a red dress smiled. Play...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets first take a look at the data we're provided with\n",
    "df = pd.read_csv(\"./challenge_set.csv\")\n",
    "print(f\"Our Dataset contains {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're given a pretty simple dataframe containing 2 columns, one containing a `url` which points to an image and the other containing a `caption` which describes the image. We want to calculate how similar the image is to the provided caption.\n",
    "\n",
    "From this a couple obvious but important insights are:\n",
    "\n",
    "1. We want to calculate the similarity between 2 inputs of different modalities (image-text).\n",
    "2. Our captions don't map to a set of predefined categories or labels E.g. [\"cat\", \"dog\", \"chair\", ...]\n",
    "\n",
    "ðŸ’¡ This highlights that we require a model that is able to classify both images and text (image-text) along with supporting the classification of unseen labels (zero-shot classification).\n",
    "\n",
    "## What is Similarity\n",
    "\n",
    "Before we dive any deeper lets give a simple example of how we'd calculate the similarity between text to text.\n",
    "\n",
    "For example, how similar are `Unleash your Creativity`, `Bob Ross` and `42`?\n",
    "\n",
    "The first thing we need is a model that is able to classify the semantic meaning of these pieces of text. These types of semantic classification or embedding models are widely available in todays rich pre-trained model ecosystem.\n",
    "\n",
    "Feeding our text through on of these models (inference) we get back an embedding. This embedding is a point in a high-dimensional vector space which encodes or represents the semantic meaning of the input text.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/text-embedding.png\" width=\"720\"/>\n",
    "</p>\n",
    "\n",
    "Continuing this process with our example pieces of text each piece of text gets a point in this vector space.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/multiple-text-embeddings.png\" width=\"720\"/>\n",
    "</p>\n",
    "\n",
    "The closeness of these points indicates their relatedness or similarity. This closeness or distance is easily measured (with measures like euclidean distance, cosine distance etc) as our embeddings are just points in vector space.\n",
    "\n",
    "ðŸ’¡ This distance measurement is our similarity score.\n",
    "\n",
    "### Comparing Images to Text\n",
    "\n",
    "What makes our problem slightly different to the above is instead of comparing text to text we're comparing images to text.\n",
    "\n",
    "Thankfully the only thing that changes is the model we are required to use to produce our classification. This model needs to be multimodal and needs to output an embedding for both images and text in the same vector space. Once we have this key piece calculating the similarity is conceptually the same as the text to text example above.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/image-text-embedding.png\" width=\"720\"/>\n",
    "</p>\n",
    "\n",
    "ðŸŽ‰ Ta-da! We now have a blueprint to conceptually calculate the similarity of an image and a piece of text.\n",
    "\n",
    "## Handling Unseen Labels\n",
    "\n",
    "As previously mentioned looking at our data reveals a wide variety of various captions and images. This freeform nature can be an issue for some models, especially if the model hasn't been trained (previously seen) on a particular label.\n",
    "\n",
    "For example, if we had a model that was only trained on images of fruit and we gave it an image of a sports car things would go south pretty quick. Therefore we need a model that's able to handle this level of variance.\n",
    "\n",
    "For this task however we don't need too go too deep on this topic, but more understand that this directly influences our model selection.\n",
    "\n",
    "More specifically we require a [Zero-shot Image Classification](https://huggingface.co/tasks/zero-shot-image-classification) model.\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "So we know we require a `Zero-shot Image Classification` model, and as previously mentioned there are a bunch of high-quality pre-trained models available for us to use.\n",
    "\n",
    "From the interim conversations I've had with the team at ðŸ§™ Leonardo.ai I understand that [ðŸ¤— Hugging Face](https://huggingface.co/) is a popular choice to use these pre-trained foundational models. So that sounds like a good place to start to me!\n",
    "\n",
    "Looking at the Zero-shot Image Classification models available a couple popular choices standout.\n",
    "\n",
    "- OpenAI's CLIP-vit models (a couple size and fine-tuned variations)\n",
    "- Meta's Metaclip models (again with a couple size and fine-tuned variations)\n",
    "\n",
    "In a real-world scenario we'd want to spend a fair amount of time comparing the performance aspects of these models and how that suits our problem requirements.\n",
    "\n",
    "For now we'll go with one of OpenAI's smaller sized variants.\n",
    "\n",
    "ðŸ‘‰ `openai/clip-vit-base-patch32` [Model Card](https://huggingface.co/openai/clip-vit-base-patch32)\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Ok so now we understand how to calculate the similarity of an image and a piece of text and we have a model in mind lets talk about implementation!\n",
    "\n",
    "I'm a big fan of the [SOLID](https://en.wikipedia.org/wiki/SOLID) principles, but I'm also very aware of not trying to \"boiling the ocean\" (over-engineering) for this particular challenge - often times I find keeping things simple goes a very long way.\n",
    "\n",
    "That being said, I'll aim for an implementation that enables easy experimentation and support for various components (models, distance calculations etc). This is something that is particularly important in todays rapidly evolving ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First thing we need to do is to convert our dataframe rows into a standard\n",
    "input format that our implementation can better work with\n",
    "\n",
    "Notably, we need to convert the image urls into actual pillow images which\n",
    "is something our implementation shouldn't be responsible for.\n",
    "\"\"\"\n",
    "inputs = [\n",
    "  Input(\n",
    "    image=load_image_from_url(row['url']),\n",
    "    caption=row['caption'],\n",
    "  ) for _, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first couple similarity scores look like: [0.43172889947891235, 0.28674182295799255, 0.2854357957839966]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Next we want to instantiate our image-text similarity calculator.\n",
    "\n",
    "As previously discussed this will use an OpenAI CLIP model to produce our\n",
    "image and text embeddings and a cosine distance calculator to calculate.\n",
    "\n",
    "These parameters are both configurable and can be swapped out for other components\n",
    "E.g. we could use a MetaCLIP model instead of OpenAI CLIP or a different distance\n",
    "calculator such as Euclidean distance.\n",
    "\"\"\"\n",
    "image_text_similarity_calculator = ImageTextSimilarityCalculator(\n",
    "  model=OpenaiClipVitModel(),\n",
    "  distance_calculator=CosineDistanceCalculator()\n",
    ")\n",
    "\n",
    "# Calculate our similarities\n",
    "similarities = image_text_similarity_calculator.calculate(inputs)\n",
    "print(f\"The first couple similarity scores look like: {similarities[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>caption</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cdn.leonardo.ai/users/85498bb1-9ae7-4b...</td>\n",
       "      <td>2 friendly real estate agent standing. one wit...</td>\n",
       "      <td>0.431729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cdn.leonardo.ai/users/b5a9a19e-f630-4e...</td>\n",
       "      <td>vector pattern, pastel colors, in style kawai ...</td>\n",
       "      <td>0.286742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://cdn.leonardo.ai/users/925ced00-c573-43...</td>\n",
       "      <td>a young beautiful girl run away kitchen. got s...</td>\n",
       "      <td>0.285436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn.leonardo.ai/users/61b0d7a9-8b0d-46...</td>\n",
       "      <td>CrianÃ§a menino de 1 ano cabelo cacheado com as...</td>\n",
       "      <td>0.271705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://cdn.leonardo.ai/users/566cd98a-7e64-47...</td>\n",
       "      <td>A little girl wearing a red dress smiled. Play...</td>\n",
       "      <td>0.369530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://cdn.leonardo.ai/users/85498bb1-9ae7-4b...   \n",
       "1  https://cdn.leonardo.ai/users/b5a9a19e-f630-4e...   \n",
       "2  https://cdn.leonardo.ai/users/925ced00-c573-43...   \n",
       "3  https://cdn.leonardo.ai/users/61b0d7a9-8b0d-46...   \n",
       "4  https://cdn.leonardo.ai/users/566cd98a-7e64-47...   \n",
       "\n",
       "                                             caption  similarity  \n",
       "0  2 friendly real estate agent standing. one wit...    0.431729  \n",
       "1  vector pattern, pastel colors, in style kawai ...    0.286742  \n",
       "2  a young beautiful girl run away kitchen. got s...    0.285436  \n",
       "3  CrianÃ§a menino de 1 ano cabelo cacheado com as...    0.271705  \n",
       "4  A little girl wearing a red dress smiled. Play...    0.369530  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we have our similarities all that's left to do is to output them\n",
    "back to our dataframe and save the output.\n",
    "\"\"\"\n",
    "df['similarity'] = similarities\n",
    "df.to_csv(\"./challenge_set_output.csv\", index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
